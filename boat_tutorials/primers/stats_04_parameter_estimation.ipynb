{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a Parameter Estimator?\n",
    "\n",
    "A parameter estimator is a function of the sample approximating a parameter of the distribution.\n",
    "\n",
    "Example: \n",
    "\n",
    "* sample mean is an estimator for the mean of the normal distribution\n",
    "* sample variance is an estimator for the variance of the normal distribution\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method of Moments Estimator\n",
    "\n",
    "In section [Parametric Distributions](./stats_03_parametric_distributions.ipynb) we used formulas based on sample statistics such as sample mean, variance, and skewness (centered moments) to obtain the model parameters. Those formulas were obtained by matching the sample moments to the theoretical moments of the distribution, and the procedure is called **method of moments**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum Likelihood Estimator\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given an observed sample $x$ and a probability model with a density function $p_{\\theta}(x)$, the likelihood function is defined as a function of the parameter:\n",
    "\n",
    "$$\\mathcal{L}(\\theta| x) = p_{\\theta}(x)$$\n",
    "\n",
    "When the sample consists of $N$ independent observations $x_1,...,x_N$, the likelihood is:\n",
    "\n",
    "$$\\mathcal{L}(\\theta| x) = \\prod_{i=1}^Np_{\\theta}(x_i)$$\n",
    "\n",
    "When the random variable is discrete, the likelihood of a parameter coincides of with the probability of observing the sample under that distribution:\n",
    "\n",
    "$$P_{\\theta}(X_1 = x_1, X_2 = x_2,..., X_N=x_N) = \\prod_{i=1}^N p_{\\theta}(x_i) = \\mathcal{L}(\\theta| x)$$\n",
    "\n",
    "We want to find the parameter $\\theta$ which maximizes this probability!\n",
    "\n",
    "$$\\hat{\\theta}_{MLE} = \\underset{\\theta}{\\operatorname*{arg max}} {\\textrm{ } \\mathcal{L}(\\theta | x)}$$\n",
    "\n",
    "This approach provides a good estimator even if the variable is continuous!\n",
    "\n",
    ":::{Note}\n",
    "\n",
    "In practice, when the sample is big to calculate the likelihood we need to compute a product of many values, which can result in a numerical error. It is equivalent ot maximize the log-likelihood which is the sum of the likelihood at each point:\n",
    "\n",
    "$$\\hat{\\theta}_{MLE} = \\underset{\\theta}{\\operatorname{arg max}}{ \\textrm{ log} \\mathcal{L}(\\theta | x)} = \\underset{\\theta}{\\operatorname{arg max}} \\sum_{i=1}^N \\log {p_{\\theta}(x_i)}$$\n",
    "\n",
    "Many probability distributions involve exponentials so that simplifies the formulas.\n",
    ":::\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Estimators\n",
    "\n",
    "**MSE** - Mean Squared Error\n",
    "\n",
    "$$ MSE(\\hat \\theta) = \\mathbb{E}_{\\theta}|\\hat \\theta - \\theta|^2$$\n",
    "\n",
    "$$MSE(\\hat \\theta) = bias(\\hat\\theta)^2 + Var(\\hat\\theta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Properties of an estimator:\n",
    "* **unbiased**: the expected value of the estimator is equal to the true parameter\n",
    "* **consistent**: as sample size grows, the estimator converges to the true parameter\n",
    "* **efficient**: has minimum variance\n",
    "\n",
    "\n",
    ":::{note}\n",
    "MLE is **Minimum Variance Unbiased Estimator** as sample size grows.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLE Widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-05-12 11:12:12--  https://docs.google.com/uc?export=download&confirm=t&id=1466snzjsXPVTlKnzkkCkdOgwoO5Zvutq\n",
      "Resolving docs.google.com (docs.google.com)... 142.251.33.78, 2607:f8b0:400a:80a::200e\n",
      "Connecting to docs.google.com (docs.google.com)|142.251.33.78|:443... connected.\n",
      "HTTP request sent, awaiting response... 303 See Other\n",
      "Location: https://drive.usercontent.google.com/download?id=1466snzjsXPVTlKnzkkCkdOgwoO5Zvutq&export=download [following]\n",
      "--2025-05-12 11:12:12--  https://drive.usercontent.google.com/download?id=1466snzjsXPVTlKnzkkCkdOgwoO5Zvutq&export=download\n",
      "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 142.251.211.225, 2607:f8b0:400a:805::2001\n",
      "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|142.251.211.225|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 5759054 (5.5M) [audio/wav]\n",
      "Saving to: ‘background.wav’\n",
      "\n",
      "background.wav      100%[===================>]   5.49M  --.-KB/s    in 0.07s   \n",
      "\n",
      "2025-05-12 11:12:14 (75.2 MB/s) - ‘background.wav’ saved [5759054/5759054]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# loading steps\n",
    "\n",
    "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&confirm=t&id=1466snzjsXPVTlKnzkkCkdOgwoO5Zvutq' -O background.wav\n",
    "\n",
    "from scipy.io import wavfile\n",
    "\n",
    "# reading background data\n",
    "samplerate, signal = wavfile.read('background.wav')\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# first we split small intervals of 0.1s\n",
    "signal_split = np.split(signal[:(len(signal)-len(signal)%samplerate)], len(signal[:(len(signal)-len(signal)%samplerate)])/samplerate*10)\n",
    "\n",
    "# we calculate RMS for each interval\n",
    "RMS_split = [np.sqrt(np.mean(np.square(group.astype('float')))) for group in signal_split]\n",
    "\n",
    "# define the r.v.\n",
    "X = np.log10(RMS_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 2.10\n",
      "Std: 0.09\n"
     ]
    }
   ],
   "source": [
    "# sample mean and variance\n",
    "mean = np.mean(X)\n",
    "std = np.std(X)\n",
    "print(f\"Mean: {mean:.2f}\")\n",
    "print(f\"Std: {std:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2385214402124535e+250\n"
     ]
    }
   ],
   "source": [
    "# gaussian likelihood\n",
    "gaussian_likelihood = stats.norm.pdf(X, loc=mean, scale=std)\n",
    "L = np.prod(gaussian_likelihood)\n",
    "print(L)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{caution}\n",
    "The likelihood evaluation involves a product of many numbers which often leads to numerical errors: either a rather big number or zero. Instead, we can maximize the logarithm of the likelihood, since log is a monotone function. The logarithm of the product is the sum of the logarithms.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "575.860191529744\n"
     ]
    }
   ],
   "source": [
    "# gaussian log-likelihood\n",
    "\n",
    "logL = np.sum(np.log(gaussian_likelihood))\n",
    "print(logL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{note} Note that even if one sample point is outside of the range of the model's probability density function, the evaluation at that point will be zero, and due to the independence assumption the whole likelihood will be zero. That in a way makes sense: if we observe a point which has probability zero for a specific model, then it means that this model has not produced this observation. In practice, there will be always outliers we may not want to fit to.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48.756884014672494\n"
     ]
    }
   ],
   "source": [
    "skewnorm_likelihood = stats.skewnorm.pdf(X, a=2, scale=std, loc=mean)\n",
    "logL = np.sum(np.log(skewnorm_likelihood))\n",
    "print(logL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`````{admonition} Exercise\n",
    "\n",
    "Plug-in the numbers from the fitting widget and compare the log-likelihood of the two distributions.\n",
    "\n",
    "`````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_skewnorm_density_L(a, scale, loc):\n",
    "  h = plt.hist(np.log10(RMS_split),bins=100, density=True, alpha=0.5)\n",
    "\n",
    "  # evaluate the function at the histogram bins\n",
    "  skewnorm_density = stats.skewnorm.pdf(h[1], a=a, scale=scale, loc=loc)\n",
    "\n",
    "  # evaluation the function at the observations\n",
    "  skewnorm_likelihood = stats.skewnorm.pdf(X, a=a, scale=scale, loc=loc)\n",
    "  L = np.sum(np.log(skewnorm_likelihood))\n",
    "\n",
    "  plt.plot(h[1], skewnorm_density)\n",
    "  plt.title(f\"Log-Likelihood {L:.10f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from ipywidgets import interact\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "shape_slider = widgets.FloatSlider(\n",
    "    value=0,\n",
    "    min=0.5,\n",
    "    max=10.0,\n",
    "    step=0.5,\n",
    "    description='Shape:',\n",
    "    #disabled=False,\n",
    "    continuous_update=True,\n",
    "    orientation='horizontal',\n",
    "    readout=True,\n",
    "    readout_format='.1f',\n",
    ")\n",
    "scale_slider = widgets.FloatSlider(\n",
    "    value=std,\n",
    "    min=std-0.1,\n",
    "    max=std+0.1,\n",
    "    step=0.01,\n",
    "    description='Scale:',\n",
    "    #disabled=False,\n",
    "    continuous_update=True,\n",
    "    orientation='horizontal',\n",
    "    readout=True,\n",
    "    readout_format='.1f',\n",
    ")\n",
    "loc_slider = widgets.FloatSlider(\n",
    "    value=mean,\n",
    "    min=mean-2,\n",
    "    max=mean+2,\n",
    "    step=0.01,\n",
    "    description='Offset:',\n",
    "    # disabled=False,\n",
    "    continuous_update=True,\n",
    "    orientation='horizontal',\n",
    "    readout=True,\n",
    "    readout_format='.1f',\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c64b84f5530c4a29af75e9f9865552a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.5, description='Shape:', max=10.0, min=0.5, readout_format='.1f', st…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "out = interact(plot_skewnorm_density_L, a = shape_slider, scale = scale_slider, loc = loc_slider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`````{admonition} Exercise\n",
    "Select a combination of parameters which \"maximizes\" the log-likelihood.\n",
    "`````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{note}\n",
    "In practice, usually the maximum likelihood estimate is found numerically through an optimization procedure. In rare cases, such as for the parameters of the Gaussian distribution distribution, it can be solved analytically and corresponds to the sample mean and standard deviation. Try to derive it yourself!\n",
    "::: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
