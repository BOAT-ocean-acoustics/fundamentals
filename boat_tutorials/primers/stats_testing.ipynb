{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Section ?? we discussed how several parametric models can \"sort of fit\" the empirical distribution. Now suppose that we want to limit the choices and we only want to compare two specific models and decide which one is \"more probable\". The two models can follow the same family of distributions but with two sets of parameters, for example, normal with two different means, or we can compare the normal distribution to logistic with fixed parameters. Based on these two options, we can design two hypotheses: \n",
    "\n",
    "$\\mathcal{H_0}: X \\sim p_0(\\theta_0),$\n",
    "\n",
    "$\\mathcal{H_1}: X \\sim p_1(\\theta_1),$\n",
    "\n",
    "where $p_0(\\theta_0)$ and $p_1(\\theta_1)$ are the two different distributions with their corrosponding parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Likehood Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given an observed sample $x$ and a probability model with a density function $p_{\\theta}(x)$, the likelihood function is defined as a function of the parameter:\n",
    "$$\\mathcal{L(\\theta| x)} = p_{\\theta}(x)$$\n",
    "\n",
    "When the sample consists of $N$ independent observations $x_1,...,x_N$, the likelihood is:\n",
    "$$\\mathcal{L(\\theta| x)} = \\prod_{i=1}^Np_{\\theta}(x_i)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is very useful in evaluating the fit of the model to some observed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p-value: the probability under the null hypothesis that we can observe "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to come up with a decision strategy based on which to determine if the data supports a certain hypothesis. A **hypothesis test** uses a **test statistic** which is a function of the sample, based on the outcome of which we can distinguish between the two hypotheses. If we assume that the null hypothesis is true, and calculate the distribution of the test statistics under the null hypothesis, we would like to reject the null hypothesis if we observe that the probability of the observed test statistic is low. The region for which the null hypothesis is rejected is called a **critical region**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we come up with such a test statistic and the cutoff for rejection? We need to accepts that we will make some errors.\n",
    "\n",
    "\n",
    "|Null hypothesis is | TRUE | FALSE|\n",
    "|-------------------|------|------|\n",
    "|Not Rejected| Correct| Type I Error |\n",
    "|Rejected| Type II Error| Correct |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability of Type I Error is called a **significance** level and usually denoted by $\\alpha$.\n",
    "\n",
    "The probability of Type II Error is denoted by $\\beta$, and the **power** of the test is $1-\\beta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To design a \"good test\" one needs to juggle both errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Likelihood Ratio Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose $X$ was a discrete variable. Then the probability of observing $x$ under each hypothesis is equal to the likelihood function of each model evaluated at $x$. So a measure of which hypothesis is \"more likely\" is the ratio of the likelihoods and we can design a test statistic based on it:\n",
    "\n",
    "$$\\Lambda(X) = \\frac{\\mathcal{L(\\theta_0|X)}}{\\mathcal{L(\\theta_1|X)}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and design a test, with a significance level $\\alpha$:\n",
    "\n",
    "Reject $H_0$:$\\Lambda(x)\\le c$,\n",
    "\n",
    "Do not reject $H_1$: $\\Lambda(x)> c$,\n",
    "\n",
    "where $c$ is chosen so that $P(\\Lambda(X)\\le c|\\mathcal{H_0}) = \\alpha$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Such a test can be specified also for continuous distributions. It turns out that *the likelihood ratio test is the most powerful test among the tests of significance level $\\alpha$*! Many of the popular tests in statistics are likelihood ratio tests.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ship Noise Detection Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to determine whether ship noise is present in the environment. If there no ship noise, we assume that the distribution of the ambient noise follows a Gaussian distribution. If there is ship noise, the distribution will be skewed. The alternative hypothesis is that the observations are from Gamma distribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$\\mathcal{H_0}: X \\sim \\mathcal{N}(\\mu_0, \\sigma^2)$\n",
    "\n",
    "$\\mathcal{H_1}: X \\sim \\mathcal{\\Gamma}(\\alpha, scale, loc)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-03-14 10:57:06--  https://docs.google.com/uc?export=download&confirm=t&id=1466snzjsXPVTlKnzkkCkdOgwoO5Zvutq\n",
      "Resolving docs.google.com (docs.google.com)... 172.217.14.206, 2607:f8b0:400a:801::200e\n",
      "Connecting to docs.google.com (docs.google.com)|172.217.14.206|:443... connected.\n",
      "HTTP request sent, awaiting response... 303 See Other\n",
      "Location: https://drive.usercontent.google.com/download?id=1466snzjsXPVTlKnzkkCkdOgwoO5Zvutq&export=download [following]\n",
      "--2025-03-14 10:57:07--  https://drive.usercontent.google.com/download?id=1466snzjsXPVTlKnzkkCkdOgwoO5Zvutq&export=download\n",
      "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 142.251.33.97, 2607:f8b0:400a:807::2001\n",
      "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|142.251.33.97|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 5759054 (5.5M) [audio/wav]\n",
      "Saving to: ‘background.wav’\n",
      "\n",
      "background.wav      100%[===================>]   5.49M  --.-KB/s    in 0.07s   \n",
      "\n",
      "2025-03-14 10:57:08 (78.5 MB/s) - ‘background.wav’ saved [5759054/5759054]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&confirm=t&id=1466snzjsXPVTlKnzkkCkdOgwoO5Zvutq' -O background.wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import wavfile\n",
    "# reading background data\n",
    "bg_samplerate, bg_signal = wavfile.read('background.wav')\n",
    "import numpy as np\n",
    "# first we split small intervals of 0.1s\n",
    "bg_signal_split = np.split(bg_signal[:(len(bg_signal)-len(bg_signal)%bg_samplerate)], len(bg_signal[:(len(bg_signal)-len(bg_signal)%bg_samplerate)])/bg_samplerate*10)\n",
    "# we calculate RMS for each interval\n",
    "RMS_split = [np.sqrt(np.mean(np.square(group.astype('float')))) for group in bg_signal_split]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will follow the following procedure:\n",
    "\n",
    "1. Set a significance level\n",
    "2. Calculate the test statistic\n",
    "3. Determine the critical region based on alpha and the test statistic\n",
    "4. Make decision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Significance level $\\alpha = 0.01$**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite some acceptance within specific fields of what a reasonable significance level is, it is important to interpret what it means in terms of the context. In this case, it means that the probability of the scientific problem. In this case, it means if we perform the experiment repeatedly, we will on average wrongly predict ship noise when it is not present $1\\%$ of the time. If we would like to detect times when ships were present in a protected zone in which they are not supposed to be present, and we would like to use the detections to notify officials if a ship is present, we would prefer to be really certain that there is a violation before do that. If we are studying the effect of noise on a another process, we could possibly incorporate that error in the follow up analysis, and it is less important about the specific choice, as long as it is small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Likelihood-ratio test statistic**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "mean = np.mean(np.log10(RMS_split))\n",
    "std = np.std(np.log10(RMS_split))\n",
    "# calculate skewness\n",
    "sk = stats.skew(np.log10(RMS_split), bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate parameters\n",
    "a = 4/(sk**2)\n",
    "# offset = mean - std*np.sqrt(a)\n",
    "offset = 1.9\n",
    "scale = std**2/(mean - offset)\n",
    "\n",
    "scale = 0.06\n",
    "a = 3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_density = stats.norm.pdf(np.log10(RMS_split), loc=mean, scale=std)\n",
    "gamma_density = stats.gamma.pdf(np.log10(RMS_split), a=a, scale=scale, loc=offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The guassian likelihood is 1.2385214402124535e+250\n",
      "The gamma likelihood is 2.2712715048394933e+259\n"
     ]
    }
   ],
   "source": [
    "print(\"The guassian likelihood is \" +str(np.prod(gaussian_density)))\n",
    "print(\"The gamma likelihood is \" +str(np.prod(gamma_density)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can hypothesize that the gamma model is more probable. But we need to test this properly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the likelihood ratio test statistic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = np.prod(gaussian_density)/np.prod(gamma_density)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.452987181732717e-10\n"
     ]
    }
   ],
   "source": [
    "print(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Identify critical region**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to determine the threshold $c$ for which the significance level of the test is $\\alpha$. Note this can be done before actually observing the sample. What we need for that is to determine the distribution of the likelihood ratio under null hypothesis distribution: in this case the Gaussian distribution with the pre-specified parameters (in this case we extracted them from the sample but we will treat them as known). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P(\\Lambda(X)\\le c)$ = $0.01$, where $X \\sim \\mathcal{N}(\\mu, \\sigma)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of trying to calculate it analytically, we will simulate a large sample from the Gaussian distribution, evaluate the likelihood ratio on it, and calculate its first percentile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate sample from the null hypothesis\n",
    "x = stats.norm.rvs(mean, std, size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9\n",
      "inf\n",
      "0.0\n",
      "0\n",
      "inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.12/site-packages/numpy/_core/fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n"
     ]
    }
   ],
   "source": [
    "# evaluate the likelihood ratio at each point\n",
    "\n",
    "print(offset)\n",
    "\n",
    "gaussian_density = stats.norm.pdf(x, loc=mean, scale=std)\n",
    "gamma_density = stats.gamma.pdf(x, a=a, scale=scale, loc=0)\n",
    "L = np.prod(gaussian_density)/np.prod(gamma_density)\n",
    "print(np.prod(gaussian_density))\n",
    "print(np.prod(gamma_density))\n",
    "print(sum(gamma_density==0))\n",
    "print(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "offset = min(np.log10(RMS_split)) - 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(1.817561646822914)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.000245354920255676)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(gaussian_density)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that even if one sample point is outside of the range of the model's probability density function, the evaluation at that point will be zero, and due to the independence assumption the whole likelihood will be zero. That in a way makes sense: if we observe a point which has probability zero for a specific model, then it means that this model has not produced this observation. In practice, there will be always out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 2\n",
    "skewnormal_density = stats.skewnorm.pdf(x, a=a, scale=std, loc=mean-0.08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inf\n",
      "inf\n",
      "nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1918/437195330.py:1: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  L = np.prod(gaussian_density)/np.prod(skewnormal_density)\n"
     ]
    }
   ],
   "source": [
    "L = np.prod(gaussian_density)/np.prod(skewnormal_density)\n",
    "print(np.prod(gaussian_density))\n",
    "print(np.prod(skewnormal_density))\n",
    "print(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9880.046020029358\n",
      "7801.742789110331\n",
      "2078.3032309190276\n"
     ]
    }
   ],
   "source": [
    "logL = np.sum(np.log(gaussian_density)) - np.sum(np.log(skewnormal_density))\n",
    "print(np.sum(np.log(gaussian_density)))\n",
    "print(np.sum(np.log(skewnormal_density)))\n",
    "print(logL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateLogL():\n",
    "  x = stats.norm.rvs(mean, std, size=len(RMS_split))\n",
    "  gaussian_density = stats.norm.pdf(x, scale=std, loc=mean)\n",
    "  skewnormal_density = stats.skewnorm.pdf(x, a=a, scale=std, loc=mean-0.08)\n",
    "  logL = np.sum(np.log(gaussian_density)) - np.sum(np.log(skewnormal_density))\n",
    "  return(logL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(62.44770745124872)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate 1 percentile\n",
    "np.percentile([evaluateLogL() for i in range(1000)], 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "575.860191529744\n",
      "48.756884014672494\n",
      "527.1033075150715\n"
     ]
    }
   ],
   "source": [
    "gaussian_density = stats.norm.pdf(np.log10(RMS_split), loc=mean, scale=std)\n",
    "skewnormal_density = stats.skewnorm.pdf(np.log10(RMS_split), a=a, scale=std, loc=mean)\n",
    "logL = np.sum(np.log(gaussian_density)) - np.sum(np.log(skewnormal_density))\n",
    "print(np.sum(np.log(gaussian_density)))\n",
    "print(np.sum(np.log(skewnormal_density)))\n",
    "print(logL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
